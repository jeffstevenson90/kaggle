{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Regression Model Steps (Preprocessing & Modeling):\n\n### 1. Evaluation & Cleaning of Data\n### 2. Feature Transformation\n### 3. Encoding\n### 4. Scaling\n### 5. Target Transformation\n### 6. Model Selection & Evaluation\n### 7. Submission","metadata":{}},{"cell_type":"code","source":"!pip install -q pycaret #for model selection library","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:42:33.144416Z","iopub.execute_input":"2021-09-10T19:42:33.14474Z","iopub.status.idle":"2021-09-10T19:43:10.063616Z","shell.execute_reply.started":"2021-09-10T19:42:33.144662Z","shell.execute_reply":"2021-09-10T19:43:10.062705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import useful packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing \n\npd.set_option('max_columns', None) # exapnd column width\npd.set_option('max_rows',81) \n\nfrom sklearn.neighbors import KNeighborsRegressor #K&N for numeric missing values\nimport scipy.stats #Feature Transformation - will only apply for numeric features\nfrom sklearn.preprocessing import StandardScaler \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom pycaret.regression import setup, compare_models #setup function will perform essential inferences about the data\n\n#import models after comparison - can run different ones if needed \nfrom catboost import CatBoostRegressor\n\nfrom sklearn.model_selection import KFold, cross_val_score #cross validation for models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-10T19:43:10.065222Z","iopub.execute_input":"2021-09-10T19:43:10.065472Z","iopub.status.idle":"2021-09-10T19:43:13.43425Z","shell.execute_reply.started":"2021-09-10T19:43:10.065443Z","shell.execute_reply":"2021-09-10T19:43:13.43363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train0 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest0 = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nsample_submission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:13.435841Z","iopub.execute_input":"2021-09-10T19:43:13.436465Z","iopub.status.idle":"2021-09-10T19:43:13.524032Z","shell.execute_reply.started":"2021-09-10T19:43:13.436425Z","shell.execute_reply":"2021-09-10T19:43:13.523458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Evaluation","metadata":{}},{"cell_type":"code","source":"train0","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:13.525722Z","iopub.execute_input":"2021-09-10T19:43:13.526135Z","iopub.status.idle":"2021-09-10T19:43:13.600442Z","shell.execute_reply.started":"2021-09-10T19:43:13.526108Z","shell.execute_reply":"2021-09-10T19:43:13.599833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use correlation heatmap viz to help see which features could be useful for a stronger correlation to salesprice:\n#OverallQual / GrLvngArea / \n\nplt.figure(figsize=[40,30])\nsns.heatmap(train0.corr(), annot = True,cmap = 'coolwarm')\nplt.xticks(fontsize=20,weight = 'bold')\nplt.yticks(fontsize=20,weight = 'bold')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:13.60156Z","iopub.execute_input":"2021-09-10T19:43:13.601939Z","iopub.status.idle":"2021-09-10T19:43:21.277278Z","shell.execute_reply.started":"2021-09-10T19:43:13.601912Z","shell.execute_reply":"2021-09-10T19:43:21.276422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test0","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.278745Z","iopub.execute_input":"2021-09-10T19:43:21.27923Z","iopub.status.idle":"2021-09-10T19:43:21.357495Z","shell.execute_reply.started":"2021-09-10T19:43:21.279194Z","shell.execute_reply":"2021-09-10T19:43:21.356632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission #End Submission Format","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.358558Z","iopub.execute_input":"2021-09-10T19:43:21.35876Z","iopub.status.idle":"2021-09-10T19:43:21.370386Z","shell.execute_reply.started":"2021-09-10T19:43:21.358738Z","shell.execute_reply":"2021-09-10T19:43:21.369447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combine Train & Test Set\n#### preprocess all data together then split before modeling","metadata":{}},{"cell_type":"code","source":"train0.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.371619Z","iopub.execute_input":"2021-09-10T19:43:21.371846Z","iopub.status.idle":"2021-09-10T19:43:21.381143Z","shell.execute_reply.started":"2021-09-10T19:43:21.371821Z","shell.execute_reply":"2021-09-10T19:43:21.380582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test0.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.381971Z","iopub.execute_input":"2021-09-10T19:43:21.38276Z","iopub.status.idle":"2021-09-10T19:43:21.393125Z","shell.execute_reply.started":"2021-09-10T19:43:21.382727Z","shell.execute_reply":"2021-09-10T19:43:21.392243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combine test and training set to get more valuable data for pre-processing.  Drop columns not needed before filling for NaN values \ntarget = train0 ['SalePrice']\ntest_ids = test0['Id']\n\ntrain1 = train0.drop(['Id', 'SalePrice'], axis=1)\ntest1 = test0.drop (['Id'], axis=1)\n\n\ndata1 = pd.concat([train1, test1], axis=0).reset_index(drop=True)\ndata1 #combined data","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.395815Z","iopub.execute_input":"2021-09-10T19:43:21.396568Z","iopub.status.idle":"2021-09-10T19:43:21.494063Z","shell.execute_reply.started":"2021-09-10T19:43:21.39653Z","shell.execute_reply":"2021-09-10T19:43:21.49323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = data1.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.495471Z","iopub.execute_input":"2021-09-10T19:43:21.495854Z","iopub.status.idle":"2021-09-10T19:43:21.500429Z","shell.execute_reply.started":"2021-09-10T19:43:21.495823Z","shell.execute_reply":"2021-09-10T19:43:21.499665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"markdown","source":"* Remove duplicate or irrelevant observations. Remove unwanted observations from your dataset, including duplicate observations or irrelevant observations\n* **Fix structural errors**\n* Filter unwanted outliers\n* **Handle missing data**\n* Validate and QA","metadata":{}},{"cell_type":"markdown","source":"## Ensure proper data types","metadata":{}},{"cell_type":"code","source":"data1.select_dtypes(np.number)#Look at all the numerical data types from combined data \n#We are looking for any numerical data type that is intended to be categorical - want to set these as strings","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.501714Z","iopub.execute_input":"2021-09-10T19:43:21.501972Z","iopub.status.idle":"2021-09-10T19:43:21.5504Z","shell.execute_reply.started":"2021-09-10T19:43:21.501945Z","shell.execute_reply":"2021-09-10T19:43:21.549595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set MSSubClass As String\ndata2['MSSubClass'] = data2['MSSubClass'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.551589Z","iopub.execute_input":"2021-09-10T19:43:21.552017Z","iopub.status.idle":"2021-09-10T19:43:21.559313Z","shell.execute_reply.started":"2021-09-10T19:43:21.551987Z","shell.execute_reply":"2021-09-10T19:43:21.558369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill Categorical Missing Values","metadata":{}},{"cell_type":"code","source":"data2.select_dtypes('object').columns #verify MSSubClass is now an object column ","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.560346Z","iopub.execute_input":"2021-09-10T19:43:21.560756Z","iopub.status.idle":"2021-09-10T19:43:21.578306Z","shell.execute_reply.started":"2021-09-10T19:43:21.560722Z","shell.execute_reply":"2021-09-10T19:43:21.577376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find categorial columns with missing values\ndata2.select_dtypes('object').loc[:,data2.isna().sum()>0].columns","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.579597Z","iopub.execute_input":"2021-09-10T19:43:21.579921Z","iopub.status.idle":"2021-09-10T19:43:21.601853Z","shell.execute_reply.started":"2021-09-10T19:43:21.57989Z","shell.execute_reply":"2021-09-10T19:43:21.600915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Look at data descriptions to figure out when 1: need to fill with the mode or 2: when missing value is intended to mean something\n\n#impute using column mode\nfor column in [\n    'MSZoning',\n    'Utilities',\n    'Exterior1st',\n    'Exterior2nd',\n    'MasVnrType',\n    'Electrical',\n    'KitchenQual',\n    'Functional',\n    'SaleType'\n]:\n    data2[column] = data2[column].fillna(data2[column].mode()[0])\n\n#2:impute using constant value\nfor column in [\n    'Alley',\n    'BsmtQual',\n    'BsmtCond',\n    'BsmtExposure',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'FireplaceQu',\n    'GarageType',\n    'GarageFinish',\n    'GarageQual',\n    'GarageCond',\n    'PoolQC',\n    'Fence',\n    'MiscFeature'\n]:\n    data2[column] = data2[column].fillna(\"None\") ","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.603565Z","iopub.execute_input":"2021-09-10T19:43:21.603896Z","iopub.status.idle":"2021-09-10T19:43:21.631634Z","shell.execute_reply.started":"2021-09-10T19:43:21.603858Z","shell.execute_reply":"2021-09-10T19:43:21.631003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.select_dtypes('object').loc[:,data2.isna().sum() > 0].columns \n#all object columns with at least one missing value for above placement ^^ -- match with data descriptions also use to validate","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.632407Z","iopub.execute_input":"2021-09-10T19:43:21.632603Z","iopub.status.idle":"2021-09-10T19:43:21.656038Z","shell.execute_reply.started":"2021-09-10T19:43:21.63258Z","shell.execute_reply":"2021-09-10T19:43:21.655481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at missing values across all columns\ndata2.select_dtypes(np.number).isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.656878Z","iopub.execute_input":"2021-09-10T19:43:21.657558Z","iopub.status.idle":"2021-09-10T19:43:21.667273Z","shell.execute_reply.started":"2021-09-10T19:43:21.657526Z","shell.execute_reply":"2021-09-10T19:43:21.66641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make a copy of data2\ndata3=data2.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.668475Z","iopub.execute_input":"2021-09-10T19:43:21.668692Z","iopub.status.idle":"2021-09-10T19:43:21.676065Z","shell.execute_reply.started":"2021-09-10T19:43:21.668669Z","shell.execute_reply":"2021-09-10T19:43:21.675276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deal with numeric missing values","metadata":{}},{"cell_type":"code","source":"#Use KNeighbors reggressor: when you have a missing value it looks at surrounding data points determine what the number should be\n#look at numeric fields with missing values:\ndata3.select_dtypes(np.number).isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.677414Z","iopub.execute_input":"2021-09-10T19:43:21.677625Z","iopub.status.idle":"2021-09-10T19:43:21.692592Z","shell.execute_reply.started":"2021-09-10T19:43:21.677602Z","shell.execute_reply":"2021-09-10T19:43:21.691805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create KNN impute function that will take in the dataframe and the column and return the same dataframe with the columns missing value filled in\ndef knn_impute (df, na_target):\n    df=df.copy() #copy of the df so the function does not modify it in place\n    \n    \n    numeric_df = df.select_dtypes(np.number) #all numeric columns\n    non_na_columns = numeric_df.loc[: ,numeric_df.isna().sum() == 0].columns #gives us the columns with no missing values \n    \n    #create temp testset that uses all the rest of the data as \"neighbors\" for missing values\n    y_train = numeric_df.loc[data3[na_target].isna() ==False, na_target] #logic to all values for NA target that do NOT have missing values in columns -- target data for regressor\n    x_train = numeric_df.loc[data3[na_target].isna() ==False, non_na_columns] #same logic but filtered to all the rest of the data \n    x_test = numeric_df.loc[data3[na_target].isna() ==True, non_na_columns] #x_test to find columns with missing values in na_target \n    \n    \n    #create actual regressor\n    knn = KNeighborsRegressor()\n    knn.fit(x_train, y_train) #fit similar data to use for prediction\n    \n    y_pred = knn.predict(x_test) #the values we will be using for imputation will come from this prediction\n    \n    df.loc[numeric_df[na_target].isna()==True, na_target] = y_pred #set all missing values in Na_Target using the predict logic\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.694049Z","iopub.execute_input":"2021-09-10T19:43:21.694341Z","iopub.status.idle":"2021-09-10T19:43:21.701375Z","shell.execute_reply.started":"2021-09-10T19:43:21.694314Z","shell.execute_reply":"2021-09-10T19:43:21.700434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_impute(data3, 'LotFrontage').isna().sum() #test to verify values for a numeric column have been filled in and regressor is working as intended","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.702432Z","iopub.execute_input":"2021-09-10T19:43:21.702852Z","iopub.status.idle":"2021-09-10T19:43:21.754391Z","shell.execute_reply.started":"2021-09-10T19:43:21.702823Z","shell.execute_reply":"2021-09-10T19:43:21.753599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.columns[data3.isna().sum() >0] #return all columns with missing values","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.75578Z","iopub.execute_input":"2021-09-10T19:43:21.756084Z","iopub.status.idle":"2021-09-10T19:43:21.77117Z","shell.execute_reply.started":"2021-09-10T19:43:21.756048Z","shell.execute_reply":"2021-09-10T19:43:21.770263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in [\n    'LotFrontage', \n    'MasVnrArea', \n    'BsmtFinSF1', \n    'BsmtFinSF2', \n    'BsmtUnfSF',\n    'TotalBsmtSF', \n    'BsmtFullBath', \n    'BsmtHalfBath', \n    'GarageYrBlt',\n    'GarageCars', \n    'GarageArea'\n]:\n    data3 = knn_impute(data3, column)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:21.772562Z","iopub.execute_input":"2021-09-10T19:43:21.772878Z","iopub.status.idle":"2021-09-10T19:43:22.01727Z","shell.execute_reply.started":"2021-09-10T19:43:21.772853Z","shell.execute_reply":"2021-09-10T19:43:22.016576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.columns.isna().sum() #No more missing values","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.018293Z","iopub.execute_input":"2021-09-10T19:43:22.018672Z","iopub.status.idle":"2021-09-10T19:43:22.024281Z","shell.execute_reply.started":"2021-09-10T19:43:22.018645Z","shell.execute_reply":"2021-09-10T19:43:22.023301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4 = data3.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.025358Z","iopub.execute_input":"2021-09-10T19:43:22.025595Z","iopub.status.idle":"2021-09-10T19:43:22.036912Z","shell.execute_reply.started":"2021-09-10T19:43:22.025572Z","shell.execute_reply":"2021-09-10T19:43:22.035767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Transformation\n### (modifying data but keeping the information)","metadata":{}},{"cell_type":"markdown","source":"## Log Transformation To Normalize Data distributions","metadata":{}},{"cell_type":"markdown","source":"#### Log functions work better when normalizing data distributions due to being able to easily unconvert and draw conclusions with the data.\n#### This does not work the same way with square rt / recipricols","metadata":{}},{"cell_type":"code","source":"#The reason to do feature transformations is because cerain models will perform better when the data it's taking is is distributed.. which is not always the case with featured data. \n#What you can do is look at the scew of a column to see where the mean is leaning. \n#We want to perform feature transformation to help correct the scew of a variable by applying a transformation to it. Will use scipy.stats for this\n\nscipy.stats.skew(data4['LotFrontage']) #this will return the scew for a given column.  Any value +/- 0.5 you can consider data as scewed and in need of transformation\n    # 0 - no scew data is perfectly distributed\n    # postive - right scewed\n    #negative - left scewed","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.038426Z","iopub.execute_input":"2021-09-10T19:43:22.038658Z","iopub.status.idle":"2021-09-10T19:43:22.049456Z","shell.execute_reply.started":"2021-09-10T19:43:22.038634Z","shell.execute_reply":"2021-09-10T19:43:22.048818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4.select_dtypes(np.number).columns","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.053486Z","iopub.execute_input":"2021-09-10T19:43:22.053993Z","iopub.status.idle":"2021-09-10T19:43:22.066898Z","shell.execute_reply.started":"2021-09-10T19:43:22.053958Z","shell.execute_reply":"2021-09-10T19:43:22.066298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logic to find the skew for each number column - User this to identify which columns need transformation\nskew_df = pd.DataFrame(data4.select_dtypes(np.number).columns, columns=[\"Feature\"])\nskew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(data4[feature]))\nskew_df['Abs Skew'] = skew_df['Skew'].apply(abs) #doesn't matter what direction the skew is going so look at abs\nskew_df['Skewed'] = skew_df['Abs Skew'].apply(lambda x: True if x >= 0.5 else False) #create column to see if data is skewed\nskew_df","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.068067Z","iopub.execute_input":"2021-09-10T19:43:22.068517Z","iopub.status.idle":"2021-09-10T19:43:22.107103Z","shell.execute_reply.started":"2021-09-10T19:43:22.068489Z","shell.execute_reply":"2021-09-10T19:43:22.106461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#return all columns with skewed data based on aboive function\nskew_df.query(\"Skewed == True\")['Feature'].values","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.108081Z","iopub.execute_input":"2021-09-10T19:43:22.108712Z","iopub.status.idle":"2021-09-10T19:43:22.118349Z","shell.execute_reply.started":"2021-09-10T19:43:22.108682Z","shell.execute_reply":"2021-09-10T19:43:22.117611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get stats for skewed data\ndata4[skew_df.query(\"Skewed == True\")['Feature'].values].describe()\n\n#issue being that we have min values at 0.. the logarithm function is undefined at 0. A useful trick for this is x+1 where the min = 0 ","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.119442Z","iopub.execute_input":"2021-09-10T19:43:22.12035Z","iopub.status.idle":"2021-09-10T19:43:22.2061Z","shell.execute_reply.started":"2021-09-10T19:43:22.120317Z","shell.execute_reply":"2021-09-10T19:43:22.205165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#numpy already has a built inlog1p function for this that will evaluate the value very close to 0\nfor column in skew_df.query(\"Skewed == True\")['Feature'].values:\n    data4[column] = np.log1p(data4[column])#applying log transformation to each for the sckewed columns ","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.207188Z","iopub.execute_input":"2021-09-10T19:43:22.207441Z","iopub.status.idle":"2021-09-10T19:43:22.231749Z","shell.execute_reply.started":"2021-09-10T19:43:22.207413Z","shell.execute_reply":"2021-09-10T19:43:22.230949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rerun logic to find the skew for each number column - Use this to identify which columns need transformation\nskew_df = pd.DataFrame(data4.select_dtypes(np.number).columns, columns=[\"Feature\"])\nskew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(data4[feature]))\nskew_df['Absolute Skew'] = skew_df['Skew'].apply(abs) #doesn't matter what direction the skew is going so look at abs\nskew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False) #create column to see if data is skewed\nskew_df","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.233044Z","iopub.execute_input":"2021-09-10T19:43:22.233265Z","iopub.status.idle":"2021-09-10T19:43:22.268329Z","shell.execute_reply.started":"2021-09-10T19:43:22.233235Z","shell.execute_reply":"2021-09-10T19:43:22.267612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nsns.distplot(data3['LotFrontage'], kde=True, fit =scipy.stats.norm) #density estimator\nplt.title(\"B4 Log Transformation\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(data3['GrLivArea'], kde=True, fit =scipy.stats.norm) #density estimator\nplt.title(\"B4 Log Transformation\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:22.269517Z","iopub.execute_input":"2021-09-10T19:43:22.269712Z","iopub.status.idle":"2021-09-10T19:43:23.034034Z","shell.execute_reply.started":"2021-09-10T19:43:22.26969Z","shell.execute_reply":"2021-09-10T19:43:23.03292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nsns.distplot(data4['LotFrontage'], kde=True, fit =scipy.stats.norm) #density estimator\nplt.title(\"With Log Transformation\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(data4['GrLivArea'], kde=True, fit =scipy.stats.norm) #density estimator\nplt.title(\"With Log Transformation\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.03579Z","iopub.execute_input":"2021-09-10T19:43:23.036126Z","iopub.status.idle":"2021-09-10T19:43:23.799142Z","shell.execute_reply.started":"2021-09-10T19:43:23.036077Z","shell.execute_reply":"2021-09-10T19:43:23.798341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5 = data4.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.800591Z","iopub.execute_input":"2021-09-10T19:43:23.801058Z","iopub.status.idle":"2021-09-10T19:43:23.806292Z","shell.execute_reply.started":"2021-09-10T19:43:23.801019Z","shell.execute_reply":"2021-09-10T19:43:23.805382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\n#### Used to add categorical columns to model","metadata":{}},{"cell_type":"code","source":"#The get_dummies() function is used to convert categorical variable into dummy/indicator variables. Data of which to get dummy indicators\n\ndata5 = pd.get_dummies(data5)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.807207Z","iopub.execute_input":"2021-09-10T19:43:23.807565Z","iopub.status.idle":"2021-09-10T19:43:23.860908Z","shell.execute_reply.started":"2021-09-10T19:43:23.807537Z","shell.execute_reply":"2021-09-10T19:43:23.860148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4.count().count()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.862128Z","iopub.execute_input":"2021-09-10T19:43:23.862349Z","iopub.status.idle":"2021-09-10T19:43:23.875852Z","shell.execute_reply.started":"2021-09-10T19:43:23.862325Z","shell.execute_reply":"2021-09-10T19:43:23.87527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.count().count()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.876983Z","iopub.execute_input":"2021-09-10T19:43:23.877402Z","iopub.status.idle":"2021-09-10T19:43:23.893065Z","shell.execute_reply.started":"2021-09-10T19:43:23.877353Z","shell.execute_reply":"2021-09-10T19:43:23.892209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data6 = data5.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.894299Z","iopub.execute_input":"2021-09-10T19:43:23.894565Z","iopub.status.idle":"2021-09-10T19:43:23.90253Z","shell.execute_reply.started":"2021-09-10T19:43:23.894539Z","shell.execute_reply":"2021-09-10T19:43:23.901454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"markdown","source":"### ML algorithms and regression models typically calucate the distance between all of the data (seperate from transformation which deals with the distribution of a single DS)\n### All features should be normalized so that each individual feature contributes an equal proportion to the model (fruit blender analogy)\n","metadata":{}},{"cell_type":"code","source":"#Research Scaling\n#This centers all the columns at 0 with a variance of 1. \n#Prediction models work more effectively when all of the features can fit on one scale\n\nscaler = StandardScaler()\nscaler.fit(data6)\ndata6 = pd.DataFrame(scaler.transform(data6), index=data6.index, columns=data6.columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.903778Z","iopub.execute_input":"2021-09-10T19:43:23.904142Z","iopub.status.idle":"2021-09-10T19:43:23.942077Z","shell.execute_reply.started":"2021-09-10T19:43:23.904104Z","shell.execute_reply":"2021-09-10T19:43:23.941389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:23.943286Z","iopub.execute_input":"2021-09-10T19:43:23.943791Z","iopub.status.idle":"2021-09-10T19:43:24.089221Z","shell.execute_reply.started":"2021-09-10T19:43:23.943752Z","shell.execute_reply":"2021-09-10T19:43:24.088612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data6","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:24.090054Z","iopub.execute_input":"2021-09-10T19:43:24.090767Z","iopub.status.idle":"2021-09-10T19:43:24.335468Z","shell.execute_reply.started":"2021-09-10T19:43:24.090739Z","shell.execute_reply":"2021-09-10T19:43:24.334883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data7 = data6.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:24.336279Z","iopub.execute_input":"2021-09-10T19:43:24.336978Z","iopub.status.idle":"2021-09-10T19:43:24.341672Z","shell.execute_reply.started":"2021-09-10T19:43:24.336949Z","shell.execute_reply":"2021-09-10T19:43:24.340923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Transformation \n#### Same as feature transformation but for our target variable","metadata":{}},{"cell_type":"code","source":"#Check to make sure min target values are above 0 before trasnformation\nnp.min(target)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:24.342687Z","iopub.execute_input":"2021-09-10T19:43:24.34292Z","iopub.status.idle":"2021-09-10T19:43:24.352511Z","shell.execute_reply.started":"2021-09-10T19:43:24.342895Z","shell.execute_reply":"2021-09-10T19:43:24.351679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Target unit tranformation has to be done seperate from feature transformation because when you trasnform the target you're essentially changing the unit \n#that the model is using for the predictions.. so if you want to analyze results be sure to undo your target trasnformations\n\n#First look at how target data is distributed\n#This chart will show how a normalized distribution will fit with the data\nplt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nsns.distplot(target, kde=True, fit =scipy.stats.norm) #density estimator\nplt.title(\"Without Log Transform\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log(target), kde=True, fit =scipy.stats.norm) #density estimator\nplt.xlabel(\"Log Sale Price\")\nplt.title(\"With Log Transform\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:24.353489Z","iopub.execute_input":"2021-09-10T19:43:24.353785Z","iopub.status.idle":"2021-09-10T19:43:25.001628Z","shell.execute_reply.started":"2021-09-10T19:43:24.353758Z","shell.execute_reply":"2021-09-10T19:43:25.000704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since we've validated that we can better distribute our target data, we will apply the transformation below\nlog_target = np.log(target)\nlog_target","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:25.002688Z","iopub.execute_input":"2021-09-10T19:43:25.002904Z","iopub.status.idle":"2021-09-10T19:43:25.009917Z","shell.execute_reply.started":"2021-09-10T19:43:25.002881Z","shell.execute_reply":"2021-09-10T19:43:25.009375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data","metadata":{}},{"cell_type":"code","source":"#We'll want to split the data so we know which rows our model is using for the train & test version\n#goal is to use the train set for the model and evaluate on the test set\ntrain_final = data7.loc[:train0.index.max(),:].copy()\ntest_final = data7.loc[train0.index.max()+1:,].reset_index(drop = True).copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:25.010788Z","iopub.execute_input":"2021-09-10T19:43:25.011646Z","iopub.status.idle":"2021-09-10T19:43:25.026671Z","shell.execute_reply.started":"2021-09-10T19:43:25.011612Z","shell.execute_reply":"2021-09-10T19:43:25.025656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection","metadata":{}},{"cell_type":"code","source":"log_target","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:25.027792Z","iopub.execute_input":"2021-09-10T19:43:25.028008Z","iopub.status.idle":"2021-09-10T19:43:25.037353Z","shell.execute_reply.started":"2021-09-10T19:43:25.027983Z","shell.execute_reply":"2021-09-10T19:43:25.036462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data to run through model selection\npd.concat([train_final, log_target], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:25.038485Z","iopub.execute_input":"2021-09-10T19:43:25.038709Z","iopub.status.idle":"2021-09-10T19:43:25.302198Z","shell.execute_reply.started":"2021-09-10T19:43:25.038685Z","shell.execute_reply":"2021-09-10T19:43:25.301356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#setup function for model selection\n_ =setup(data=pd.concat([train_final, log_target], axis=1), target='SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:25.303171Z","iopub.execute_input":"2021-09-10T19:43:25.303438Z","iopub.status.idle":"2021-09-10T19:43:35.377579Z","shell.execute_reply.started":"2021-09-10T19:43:25.303409Z","shell.execute_reply":"2021-09-10T19:43:35.376849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models()\n#each model takes a different approach at solving the prediction most are regression models. Ideally looking for top reccomendation and lowest RMSE","metadata":{"execution":{"iopub.status.busy":"2021-09-10T19:43:35.378733Z","iopub.execute_input":"2021-09-10T19:43:35.379358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"code","source":"baseline_model = CatBoostRegressor(verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model.fit(train_final, log_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"#logic to cross validate prediction results against the train_final\n#Will make 10 different splits, cross_val score will return results from each run --> results will contain a list of NMSE from the cross validations\nKf = KFold(n_splits=10)\n\nresults = cross_val_score(baseline_model, train_final, log_target, scoring='neg_mean_squared_error', cv=Kf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the mean sale price from model.  Do this by un-transforming the target metric \nnp.exp(np.sqrt(np.mean(-results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validate forecast model against training actuals\n\nTrain_Prediction = np.exp(baseline_model.predict(train_final)) #remember to un-transform values for predictions\n\ntp = pd.concat([test_ids,target,pd.Series(Train_Prediction, name = 'SalePricePredict')],axis=1)\n#Initial model results look very good.  We have a RMSE *avg error against prediction & actuals* of 1.12 out of a very wide range of sales prices\n\nprint(tp)\n\nplt.figure(figsize=(40,15))\n\nx = tp.Id\ny = tp.SalePrice\n\nx2 = tp.Id\ny2 = tp.SalePricePredict\n\nplt.plot(x, y, label='Actual')\nplt.plot(x2, y2, label='Prediction')\nplt.legend(loc='upper center')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = np.exp(baseline_model.predict(test_final)) #remember to un-transform values for predictions\n\nsubmission = pd.concat([test_ids, pd.Series(final_predictions, name = 'SalePrice')], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('./submission.csv',index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}